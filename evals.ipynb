{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, PaliGemmaForConditionalGeneration, PaliGemmaProcessor,  BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model_id = \"google/paligemma2-3b-pt-896\"\n",
    "model = PaliGemmaForConditionalGeneration.from_pretrained(model_id,\n",
    "                                                         quantization_config=bnb_config,\n",
    "                                                         device_map={\"\":3})\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\", 'answer': {'columns': ['Name', 'Num_Employees'], 'index': [0, 1, 2], 'data': [['Treasury', 115897.0], ['Homeland Security', 208000.0], ['Treasury', 115897.0]]}, 'table_names': ['department', 'management'], 'table_image_ids': ['TableImg_1qdjq_15.png', 'TableImg_56w5t_5.png'], 'original_data_index': 0}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open('DataVLM/mmqa_extended/two_tables/mmqa_extended_two_tables.jsonl') as f:\n",
    "    for line in map(str.strip, f):\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            item = json.loads(line)\n",
    "            data.append(item)\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "        \n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image \n",
    "import torch\n",
    "images = data[0]['table_image_ids']\n",
    "\n",
    "image2 = Image.open(f\"DataVLM/mmqa_extended/two_tables/table_images/{data[0]['table_image_ids'][0]}\").resize((448,448))\n",
    "img1 = Image.open(f\"DataVLM/mmqa_extended/two_tables/table_images/{data[0]['table_image_ids'][1]}\").resize((448, 448))\n",
    "\n",
    "prompt = f\" Provide natural language answer to the question: {data[0]['question']} \\n answer:\"\n",
    "text=prompt\n",
    "inputs = processor(images=[[image2, img1]], text=prompt, return_tensors=\"pt\").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.input_ids.shape, inputs.attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(processor.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.1 environment at: vlmenv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 5ms\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=\"AIzaSyBMLGJr-55FedA7vul19WYPbKpfIpg1I5w\")\n",
    "\n",
    "def upload_to_gemini(path, mime_type=None):\n",
    "  \"\"\"Uploads the given file to Gemini.\n",
    "\n",
    "  See https://ai.google.dev/gemini-api/docs/prompting_with_media\n",
    "  \"\"\"\n",
    "  file = genai.upload_file(path, mime_type=mime_type)\n",
    "  print(f\"Uploaded file '{file.display_name}' as: {file.uri}\")\n",
    "  return file\n",
    "\n",
    "\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "\n",
    "SYSTEMS_INSTRUCTIONS = \"\"\"\n",
    "You are an intelligent assistant capable of understanding and reasoning about tabular data. You will be presented with one or more tables containing information on a specific topic. \n",
    "You will then be asked a question that requires you to analyze the data in the table(s) and provide a correct answer.\n",
    "\\n**Your task is to:**\\n\\n\n",
    "1. Carefully examine the provided table(s).Pay close attention to the column headers, the data types within each column, and the relationships between tables if multiple tables are given.\n",
    "2. Understand the question being asked. Identify the specific information being requested and determine which table(s) and columns are relevant to answering the question.\n",
    "3. Extract the necessary information from the table(s).** Perform any required filtering, joining, aggregation, or calculations on the data to arrive at the answer.\n",
    "4. Formulate a clear and concise answer in natural language.** The answer should be directly responsive to the question and presented in a human-readable format. It may involve listing data, presenting a single value, or explaining a derived insight.\n",
    "5. Do not include any SQL queries in the answer.** Your response should be in natural language only, as if you were explaining the answer to a human.\n",
    "6. Be accurate and avoid hallucinations.** Your answer should be completely based on the data in the provided table(s). Do not introduce any external information or make assumptions not supported by the data.\n",
    "7. Be specific and follow the instructions in the question.** If the question ask to get specific columns, return only mentioned columns, otherwise return all columns.\n",
    "8. If the question is unanswerable** based on the provided tables, state \"The question cannot be answered based on the provided data.\n",
    "9. Give answer in json format like this { ['ans1', ans1], ['ans1', ans1], ['ans1', ans1] }\",\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-exp-1206\",\n",
    "  generation_config=generation_config,\n",
    "  system_instruction=SYSTEMS_INSTRUCTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded file 'TableImg_1qdjq_15.png' as: https://generativelanguage.googleapis.com/v1beta/files/sfz3oxyy0ti3\n",
      "Uploaded file 'TableImg_56w5t_5.png' as: https://generativelanguage.googleapis.com/v1beta/files/pxy7e5l5bdhz\n",
      "```json\n",
      "{\n",
      "  \"ans\": [\n",
      "    [\n",
      "      \"Treasury\",\n",
      "      \"115897.000000\"\n",
      "    ],\n",
      "    [\n",
      "      \"Homeland Security\",\n",
      "      \"208000.000000\"\n",
      "    ]\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Uploaded file 'TableImg_1ft36_15.png' as: https://generativelanguage.googleapis.com/v1beta/files/9tb6w4owgpqe\n",
      "Uploaded file 'TableImg_Sab76_5.png' as: https://generativelanguage.googleapis.com/v1beta/files/5xcvh4jjxskm\n",
      "```json\n",
      "{\n",
      "  \"ans\": [\n",
      "    \"10\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Uploaded file 'TableImg_34u3i_10.png' as: https://generativelanguage.googleapis.com/v1beta/files/12hufwltz61t\n",
      "Uploaded file 'TableImg_3lekt_5.png' as: https://generativelanguage.googleapis.com/v1beta/files/7c4hbx6i4nn3\n",
      "```json\n",
      "{\n",
      "  \"ans\": [\n",
      "    \"53.000000\",\n",
      "    \"52.000000\",\n",
      "    \"69.000000\"\n",
      "  ]\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "with open(\"evals/results/my_results.jsonl\", \"w\", encoding=\"utf-8\") as out_file:\n",
    "  for i, row in enumerate(tqdm(data)):\n",
    "    files = [\n",
    "      upload_to_gemini(f\"DataVLM/mmqa_extended/two_tables/table_images/{img}\", mime_type=\"image/png\")\n",
    "      for img in row[\"table_image_ids\"]\n",
    "    ]\n",
    "    prompt = f\"Question: {row['question']} \\n answer:\"\n",
    "    chat_session = model.start_chat(\n",
    "      history=[\n",
    "        {\n",
    "          \"role\": \"user\",\n",
    "          \"parts\": files,\n",
    "        },\n",
    "      ],\n",
    "    )\n",
    "    response = chat_session.send_message(prompt)\n",
    "    \n",
    "    result = {\n",
    "      \"question\": row[\"question\"],\n",
    "      \"golden_answer\": row[\"answer\"],\n",
    "      \"table_image_ids\": row[\"table_image_ids\"],\n",
    "      \"gemini_response\": response.text,\n",
    "    }\n",
    "    out_file.write(json.dumps(result) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How many departments are led by heads who are not mentioned?',\n",
       " 'answer': {'columns': ['count(*)'], 'index': [0], 'data': [[11]]},\n",
       " 'table_names': ['department', 'management'],\n",
       " 'table_image_ids': ['TableImg_1ft36_15.png', 'TableImg_Sab76_5.png'],\n",
       " 'original_data_index': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"parts\": [\n",
    "        files[0],\n",
    "        files[1],\n",
    "        \"Question: Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\\n\\nAnswer:\",\n",
    "      ],\n",
    "    },\n",
    "  ]\n",
    ")\n",
    "\n",
    "response = chat_session.send_message(\"INSERT_INPUT_HERE\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "def upload_to_gemini(path, mime_type=None):\n",
    "  \"\"\"Uploads the given file to Gemini.\n",
    "\n",
    "  See https://ai.google.dev/gemini-api/docs/prompting_with_media\n",
    "  \"\"\"\n",
    "  file = genai.upload_file(path, mime_type=mime_type)\n",
    "  print(f\"Uploaded file '{file.display_name}' as: {file.uri}\")\n",
    "  return file\n",
    "\n",
    "# Create the model\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 64,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-exp-1206\",\n",
    "  generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# TODO Make these files available on the local file system\n",
    "# You may need to update the file paths\n",
    "files = [\n",
    "  upload_to_gemini(\"TableImg_1qdjq_15.png\", mime_type=\"image/png\"),\n",
    "  upload_to_gemini(\"TableImg_56w5t_5.png\", mime_type=\"image/png\"),\n",
    "]\n",
    "\n",
    "chat_session = model.start_chat(\n",
    "  history=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"parts\": [\n",
    "        files[0],\n",
    "        files[1],\n",
    "        \"Provide natural language answer to the question : Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'? Answer: \",\n",
    "      ],\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"model\",\n",
    "      \"parts\": [\n",
    "        \"The departments managed by heads with a temporary acting value of 'Yes' are the **Treasury** department, which has **115897.0** employees, and the **Homeland Security** department, which has **208000.0** employees.\",\n",
    "      ],\n",
    "    },\n",
    "  ]\n",
    ")\n",
    "\n",
    "response = chat_session.send_message(\"INSERT_INPUT_HERE\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/0052a70beed5bf71b92610a43a52df6d286cd5f3/diffusers/rabbit.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "prompt = \"<|image|><|image|><|begin_of_text|>If I had to write a haiku for this one\"\n",
    "inputs = processor(image, prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "output = model.generate(**inputs, max_new_tokens=30)\n",
    "print(processor.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.13.1 environment at: vlmenv\u001b[0m\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m9 packages\u001b[0m \u001b[2min 484ms\u001b[0m\u001b[0m                                         \u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m2 packages\u001b[0m \u001b[2min 1.67s\u001b[0m\u001b[0m                                             \n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m2 packages\u001b[0m \u001b[2min 51ms\u001b[0m\u001b[0m                                \u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mav\u001b[0m\u001b[2m==14.0.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mqwen-vl-utils\u001b[0m\u001b[2m==0.0.8\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! uv pip install qwen-vl-utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"Show the name and number of employees for the departments managed by heads whose temporary acting value is 'Yes'?\", 'answer': {'columns': ['Name', 'Num_Employees'], 'index': [0, 1, 2], 'data': [['Treasury', 115897.0], ['Homeland Security', 208000.0], ['Treasury', 115897.0]]}, 'table_names': ['department', 'management'], 'table_image_ids': ['TableImg_1qdjq_15.png', 'TableImg_56w5t_5.png'], 'original_data_index': 0}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open('DataVLM/mmqa_extended/two_tables/mmqa_extended_two_tables.jsonl') as f:\n",
    "    for line in map(str.strip, f):\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            item = json.loads(line)\n",
    "            data.append(item)\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "        \n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc813225fe04033820fe7ff71dcfb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[\\n    {\\n        \"name\": \"State\",\\n        \"num_employees\": 3026600000\\n    },\\n    {\\n        \"name\": \"Justice\",\\n        \"num_employees\": 11255700000\\n    },\\n    {\\n        \"name\": \"Labor\",\\n        \"num_employees\": 17347000000\\n    },\\n    {\\n        \"name\": \"Veterans Affairs\",\\n        \"num_employees\": 23500000000\\n    }\\n]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor, BitsAndBytesConfig\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "# # default: Load the model on the available device(s)\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",torch_dtype=torch.bfloat16).to(\"cuda:3\")\n",
    "\n",
    "# Load the model on the specified device(s) \n",
    "\n",
    "\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "# model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     attn_implementation=\"flash_attention_2\",\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-2B-Instruct\")\n",
    "\n",
    "# The default range for the number of visual tokens per image in the model is 4-16384. You can set min_pixels and max_pixels according to your needs, such as a token count range of 256-1280, to balance speed and memory usage.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": Image.open(f\"DataVLM/mmqa_extended/two_tables/table_images/{data[0]['table_image_ids'][0]}\").resize((448, 448)),\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": Image.open(f\"DataVLM/mmqa_extended/two_tables/table_images/{data[1]['table_image_ids'][0]}\").resize((448, 448)),\n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": f\"{SYSTEMS_INSTRUCTIONS}  \\n Question: {data[0]['question']}  \\nAnswer: \"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "image_inputs, Video = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(model.device)\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1000)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEMS_INSTRUCTIONS = \"\"\"\n",
    "You are an intelligent assistant capable of understanding and reasoning about tabular data. You will be presented with one or more tables containing information on a specific topic. \n",
    "You will then be asked a question that requires you to analyze the data in the table(s) and provide a correct answer.\n",
    "\\n**Your task is to:**\\n\\n\n",
    "1. Carefully examine the provided table(s).Pay close attention to the column headers, the data types within each column, and the relationships between tables if multiple tables are given.\n",
    "2. Understand the question being asked. Identify the specific information being requested and determine which table(s) and columns are relevant to answering the question.\n",
    "3. Extract the necessary information from the table(s).** Perform any required filtering, joining, aggregation, or calculations on the data to arrive at the answer.\n",
    "4. Formulate a clear and concise answer in natural language.** The answer should be directly responsive to the question and presented in a human-readable format. It may involve listing data, presenting a single value, or explaining a derived insight.\n",
    "5. Do not include any SQL queries in the answer.** Your response should be in natural language only, as if you were explaining the answer to a human.\n",
    "6. Be accurate and avoid hallucinations.** Your answer should be completely based on the data in the provided table(s). Do not introduce any external information or make assumptions not supported by the data.\n",
    "7. Be specific and follow the instructions in the question.** If the question ask to get specific columns, return only mentioned columns, otherwise return all columns.\n",
    "8. If the question is unanswerable** based on the provided tables, state \"The question cannot be answered based on the provided data.\n",
    "9. Give answer directly in json format.,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 21 23:12:07 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA TITAN X (Pascal)        Off |   00000000:02:00.0 Off |                  N/A |\n",
      "| 23%   25C    P8              8W /  250W |   11955MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA TITAN X (Pascal)        Off |   00000000:03:00.0 Off |                  N/A |\n",
      "| 23%   28C    P8              9W /  250W |    4149MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA TITAN X (Pascal)        Off |   00000000:82:00.0 Off |                  N/A |\n",
      "| 23%   30C    P8             10W /  250W |   11793MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA TITAN X (Pascal)        Off |   00000000:83:00.0 Off |                  N/A |\n",
      "| 30%   49C    P5             16W /  250W |    6091MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    345487      C   ...nwar/anaconda3/envs/scot/bin/python      11952MiB |\n",
      "|    1   N/A  N/A    400803      C   python3                                      4146MiB |\n",
      "|    2   N/A  N/A    776775      C   python3                                      8208MiB |\n",
      "|    2   N/A  N/A    777046      C   python3                                      3582MiB |\n",
      "|    3   N/A  N/A   3790572      C   ...gh/tableVLM/evals/vlenv/bin/python3       6088MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Ignored the following yanked versions: 0.1.6, 0.1.7, 0.1.8, 0.1.9, 0.2.0, 0.2.1, 0.2.2, 0.2.2.post2, 0.2.2.post3\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchvision==0.17.0 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchvision==0.17.0\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchvision==0.17.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 21 23:09:49 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA TITAN X (Pascal)        Off |   00000000:02:00.0 Off |                  N/A |\n",
      "| 23%   25C    P8              8W /  250W |   11955MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA TITAN X (Pascal)        Off |   00000000:03:00.0 Off |                  N/A |\n",
      "| 23%   28C    P8              9W /  250W |    4149MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA TITAN X (Pascal)        Off |   00000000:82:00.0 Off |                  N/A |\n",
      "| 23%   30C    P8             10W /  250W |   11793MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA TITAN X (Pascal)        Off |   00000000:83:00.0 Off |                  N/A |\n",
      "| 28%   52C    P2             83W /  250W |   11089MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    345487      C   ...nwar/anaconda3/envs/scot/bin/python      11952MiB |\n",
      "|    1   N/A  N/A    400803      C   python3                                      4146MiB |\n",
      "|    2   N/A  N/A    776775      C   python3                                      8208MiB |\n",
      "|    2   N/A  N/A    777046      C   python3                                      3582MiB |\n",
      "|    3   N/A  N/A   3790367      C   ...gh/tableVLM/evals/vlenv/bin/python3      11086MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!kill 3790572"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jan 21 23:10:08 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA TITAN X (Pascal)        Off |   00000000:02:00.0 Off |                  N/A |\n",
      "| 23%   25C    P8              8W /  250W |   11955MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA TITAN X (Pascal)        Off |   00000000:03:00.0 Off |                  N/A |\n",
      "| 23%   28C    P8              9W /  250W |    4149MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA TITAN X (Pascal)        Off |   00000000:82:00.0 Off |                  N/A |\n",
      "| 23%   30C    P8             10W /  250W |   11793MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA TITAN X (Pascal)        Off |   00000000:83:00.0 Off |                  N/A |\n",
      "| 29%   55C    P0             87W /  250W |       3MiB /  12288MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A    345487      C   ...nwar/anaconda3/envs/scot/bin/python      11952MiB |\n",
      "|    1   N/A  N/A    400803      C   python3                                      4146MiB |\n",
      "|    2   N/A  N/A    776775      C   python3                                      8208MiB |\n",
      "|    2   N/A  N/A    777046      C   python3                                      3582MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "llm"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
